{"cells":[{"cell_type":"markdown","source":["# Reinforcement Learning: Monte Carlo Prediction \n","\n","very kindly version"],"metadata":{"id":"T5Uqx-hebFLy"}},{"cell_type":"markdown","source":["# Google Drive와 연동하기"],"metadata":{"id":"Sac4JLNcK2ew"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"jrhxZLF7KMrc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a69f087-df9a-4ea7-dd01-5642d064c1e7","executionInfo":{"status":"ok","timestamp":1679993733494,"user_tz":-540,"elapsed":23583,"user":{"displayName":"‍황효석(소프트웨어융합학과)","userId":"13124552253495440690"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qvVtShMMKKFo"},"outputs":[],"source":["# 본인의 파일 저장 경로로 변경해주세요\n","%cd /content/drive/MyDrive/'Colab Notebooks'/RL2023/MC_Prediction\n","!ls\n","import sys; sys.path.append('..') # add project root to the python path\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from gridworld import *\n","\n","np.random.seed(0)"]},{"cell_type":"markdown","metadata":{"id":"W7pIhjRYKKFp"},"source":["## `GridWorld` 초기화하기\n","\n","가로로 `nx` 개, 세로로 `ny` 개의 칸을 가진 `GridworldEnv`를 만듭니다!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"162cx8ygKKFq"},"outputs":[],"source":["nx, ny = 5, 5\n","env = GridworldEnv([ny, nx])"]},{"cell_type":"markdown","source":["def get_action(state): 현재 state에서 action을 정의하는 함수. 본 예제에서는 uniform random 으로 정의합니다. (즉, 모든 방향의 확률이 동일함)"],"metadata":{"id":"adgK5LWYbreq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rLia2qXRKKFr"},"outputs":[],"source":["def get_action(state):\n","\n","        action = np.random.choice(range(4))\n","\n","        return action"]},{"cell_type":"markdown","metadata":{"id":"uLXMCeJDKKFr"},"source":["###  `run_episode()`\n","\n","MC 기법은 하나의 에피소드가 진행되고 state,action,reward 정보를 모두 모아 반환합니다.\n"]},{"cell_type":"code","source":["def run_episode(env, timeout=1000):\n","    env.reset()\n","    states = []\n","    actions = []\n","    rewards = []\n","\n","    i = 0\n","    timeouted = False\n","    while True:\n","        state = env.s\n","        action = get_action(state)\n","        next_state, reward, done, info = env.step(action)\n","        states.append(state)\n","        actions.append(action)\n","        rewards.append(reward)\n","\n","        if done:\n","            break\n","        else:\n","            i += 1\n","            if i >= timeout:\n","                timeouted = True\n","                break\n","\n","    if not timeouted:\n","        episode = (states, actions, rewards)\n","    return episode"],"metadata":{"id":"8V8-8qsWMzcb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["no_episode 개를 생성하여 episodes 에 저장합니다.\n","\n","기본 1000개이지만 샘플 숫자를 바꿔가면서 해보세요."],"metadata":{"id":"st7QA3u8b-Zy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q5LaW6i1KKFs"},"outputs":[],"source":["episodes=[]\n","no_episode = 10000\n","for _ in range(no_episode):\n","    episodes.append(run_episode(env))"]},{"cell_type":"markdown","source":["에피소드의 형식 및 transit이 궁금하면 아래의 코드를 실행해보세요\n","\n","episode는 s1, a1, r1, s2, a2, r2... 순으로 저장되어 있습니다."],"metadata":{"id":"FeHgqHwEcWy6"}},{"cell_type":"code","source":["#  episodes 는 list 이므로, len으로 크기 확인\n","print(len(episodes))\n","# 10개만 출력해봅시다\n","print(episodes[1:10])\n"],"metadata":{"id":"zSynA5TecXAx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MC\n","$$V(s) \\leftarrow \\frac{S(s)}{N(s)}$$\n","$$Q(s,a) \\leftarrow \\frac{S(s,a)}{N(s,a)}$$\n","\n","$S(s)$ : 상태 s 에 대한 return들의 합 \\\\\n","$N(s)$ : 상태 s 를 방문한 횟수 \\\\\n","$S(s,a)$ : 상태 s에서 a 행동을 했을 때 return들의 값 \\\\\n","$N(s,a)$ : 상태 s에서 a 행동을 한 횟수\n","\n","Every Visit 방식입니다."],"metadata":{"id":"ahLce3UrIR57"}},{"cell_type":"code","source":["s_v=np.zeros(shape=nx*ny)\n","s_q = np.zeros(shape=(nx*ny,4))\n","n_v=np.zeros(shape=nx*ny)\n","n_q = np.zeros(shape=(nx*ny,4))\n","gamma=1.0\n","lr=1e-3\n","\n","for episode in episodes:\n","    states, actions, rewards = episode\n","\n","    # reversing the inputs!\n","    # for efficient computation of returns\n","    states = reversed(states)\n","    actions = reversed(actions)\n","    rewards = reversed(rewards)\n","\n","    iter = zip(states, actions, rewards)\n","    cum_r = 0\n","    for s, a, r in iter:\n","        cum_r *= gamma\n","        cum_r += r\n","\n","        n_v[s] += 1\n","        n_q[s, a] += 1\n","\n","        s_v[s] += cum_r\n","        s_q[s, a] += cum_r\n","v = s_v / (n_v)\n","q = s_q / (n_q)"],"metadata":{"id":"ymcA7JyxISQ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 추산된 상태가치함수 $V(s)$ 및 $Q(s,a)$ 확인하기"],"metadata":{"id":"BXmDCOCHISl9"}},{"cell_type":"code","source":["fig, ax = plt.subplots(1,2, figsize=(12,6))\n","visualize_value_function(ax[0], v, nx, ny)\n","_ = ax[0].set_title(\"Value pi\")\n","visualize_policy(ax[1], q, nx, ny)\n","_ = ax[1].set_title(\"Greedy policy\")"],"metadata":{"id":"4Xj-kjMDIS9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","def highlight_max(s):\n","    '''\n","    highlight the maximum in a Series green.\n","    '''\n","    is_max = s == s.max()\n","    return ['background-color: green' if v else '' for v in is_max]\n","\n","def visualize_q(q):\n","    df = pd.DataFrame(q, columns=['up', 'right', 'down', 'left']).T\n","    df = df.style.apply(highlight_max)\n","    return df\n","df = visualize_q(q)\n","df"],"metadata":{"id":"2AjJ9_cNITSR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Incremental MC (Mission)\n","\n","\n","$$V(s) \\leftarrow V(s) + \\alpha (G(s) - V(s))$$\n","$$Q(s,a) \\leftarrow Q(s,a) + \\alpha (G(s,a) - Q(s,a))$$\n","\n","$\\alpha$: learning rate \\\\\n","$G(s)$:상태 s에서의 return \\\\\n","$G(s,a)$: 상태 s, 행동 a에서의 return\n","\n","alpha = 값을 정의한 후 incremental MC 방식으로 v(s)와 q(s,a)의 값을 계산해보세요"],"metadata":{"id":"oUGcQdOeHtjQ"}},{"cell_type":"code","source":["v=np.zeros(shape=nx*ny)\n","q = np.zeros(shape=(nx*ny,4))\n","gamma=1.0\n","alpha=1e-3\n","\n","# ------------------- MISSION ---------------------\n","# 여기에 코드를 작성하세요. 위의 코드를 이용하세요."],"metadata":{"id":"O8zPxybKHt5U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-d7FdND7KKFt"},"source":["## 추산된 상태가치함수 $V(s)$ 및 $Q(s,a)$ 확인하기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INvCdQdOKKFt"},"outputs":[],"source":["fig, ax = plt.subplots(1,2, figsize=(12,6))\n","visualize_value_function(ax[0], v, nx, ny)\n","_ = ax[0].set_title(\"Value pi\")\n","visualize_policy(ax[1], q, nx, ny)\n","_ = ax[1].set_title(\"Greedy policy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdLqiJ3WKKFt"},"outputs":[],"source":["import pandas as pd\n","\n","def highlight_max(s):\n","    '''\n","    highlight the maximum in a Series green.\n","    '''\n","    is_max = s == s.max()\n","    return ['background-color: green' if v else '' for v in is_max]\n","\n","def visualize_q(q):\n","    df = pd.DataFrame(q, columns=['up', 'right', 'down', 'left']).T\n","    df = df.style.apply(highlight_max)\n","    return df\n","df = visualize_q(q)\n","df"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}