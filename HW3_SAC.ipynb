{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kWWWrvymcrWv"
      },
      "outputs": [],
      "source": [
        "# !pip install swig\n",
        "# !pip install gymnasium\n",
        "# !pip install gymnasium[box2d]\n",
        "# !pip install imageio\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import cv2\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "import glob\n",
        "import base64, io\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQN9qVCwxCns",
        "outputId": "3e7ae5e2-4440-4ebc-fb46-540bc93ff574"
      },
      "outputs": [],
      "source": [
        "def preprocess_image(img):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Grayscale(),\n",
        "        transforms.Resize((84, 84)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    img = transform(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "class ImageEnv(gym.Wrapper):\n",
        "    def __init__(\n",
        "            self,\n",
        "            env,\n",
        "            stack_frames=3,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super(ImageEnv, self).__init__(env, **kwargs)\n",
        "        self.stack_frames = stack_frames\n",
        "        self.skip_frames = 1\n",
        "    def reset(self):\n",
        "        # Reset the original environment.\n",
        "        self.env.reset()\n",
        "\n",
        "        s =preprocess_image(self.env.render())\n",
        "\n",
        "        # The initial observation is simply a copy of the frame `s`\n",
        "        self.stacked_state = s.repeat(self.stack_frames, 1, 1)\n",
        "        return self.stacked_state\n",
        "\n",
        "    def step(self, action):\n",
        "        # We take an action for self.skip_frames steps\n",
        "        reward = 0\n",
        "        for _ in range(self.skip_frames):\n",
        "            s, r, terminated, truncated, info = self.env.step(action)\n",
        "            reward += r\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        # Convert a frame to 84 X 84 gray scale one\n",
        "        s =preprocess_image(self.env.render())\n",
        "\n",
        "        # Push the current frame `s` at the end of self.stacked_state\n",
        "        self.stacked_state=torch.cat((self.stacked_state[1:],s),dim=0)\n",
        "        return self.stacked_state, reward, terminated, truncated, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMIKQmQ5fxrA"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "NCPK0s5Ud8Bf"
      },
      "outputs": [],
      "source": [
        "# 네트워크는 자유롭게 구성\n",
        "# 아래는 예시\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, actor_lr):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "\n",
        "        self.conv1=nn.Conv2d(state_dim[0],16,kernel_size=8,stride=4) #[N,4,84,84] -> [N,16,20,20]\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)  # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
        "        self.in_features=32*9*9\n",
        "        self.fc1=nn.Linear(self.in_features,256)\n",
        "        self.fc_mu=nn.Linear(256,action_dim)\n",
        "        self.fc_std=nn.Linear(256,action_dim)\n",
        "\n",
        "        self.lr = actor_lr\n",
        "        self.LOG_STD_MIN = -20\n",
        "        self.LOG_STD_MAX = 2\n",
        "        self.max_action = 2\n",
        "        self.min_action = -2\n",
        "        self.action_scale = (self.max_action - self.min_action) / 2.0\n",
        "        self.action_bias = (self.max_action + self.min_action) / 2.0\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv1(x))\n",
        "        x = F.leaky_relu(self.conv2(x))\n",
        "        x = x.view(-1, self.in_features)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        mu = self.fc_mu(x)\n",
        "        log_std = self.fc_std(x)\n",
        "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
        "        return mu, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        mean, log_std = self.forward(state)\n",
        "        std = torch.exp(log_std)\n",
        "        reparameter = Normal(mean, std)\n",
        "        x_t = reparameter.rsample()\n",
        "        y_t = torch.tanh(x_t)\n",
        "        action = self.action_scale * y_t + self.action_bias\n",
        "\n",
        "        # # Enforcing Action Bound\n",
        "        log_prob = reparameter.log_prob(x_t)\n",
        "        log_prob = log_prob - torch.sum(torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6), dim=-1, keepdim=True)\n",
        "\n",
        "        return action, log_prob\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, critic_lr):\n",
        "        super(QNetwork, self).__init__()\n",
        "        \n",
        "        # First define convolutional layers to process the image state\n",
        "        self.conv1 = nn.Conv2d(state_dim[0], 16, kernel_size=8, stride=4)  # [N, 3, 84, 84] -> [N, 16, 20, 20]\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)  # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
        "        self.in_features = 32 * 9 * 9  # Flatten the output of the last conv layer to feed into a linear layer\n",
        "\n",
        "        # Linear layers to process the flattened conv output\n",
        "        self.fc1 = nn.Linear(self.in_features, 256)\n",
        "\n",
        "        # Separate processing paths for state and action inputs\n",
        "        self.fc_s = nn.Linear(256, 128)  # Process state features\n",
        "        self.fc_a = nn.Linear(action_dim, 128)  # Process actions\n",
        "\n",
        "        # Combine state and action processing paths\n",
        "        self.fc_cat = nn.Linear(256, 128)  # Layer to combine state and action features\n",
        "        self.fc_out = nn.Linear(128, 1)  # Output layer to produce a single Q-value\n",
        "\n",
        "        # Learning rate and optimizer\n",
        "        self.lr = critic_lr\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=self.lr)\n",
        "\n",
        "    def forward(self, x, a):\n",
        "        # Process the image input through convolutional layers\n",
        "        x = F.leaky_relu(self.conv1(x))\n",
        "        x = F.leaky_relu(self.conv2(x))\n",
        "        x = x.view(-1, self.in_features)  # Flatten the output for the linear layer\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "\n",
        "        # Further process state features\n",
        "        s = F.leaky_relu(self.fc_s(x))\n",
        "        \n",
        "        # Process action features\n",
        "        a = F.leaky_relu(self.fc_a(a))\n",
        "\n",
        "        # Concatenate state and action features\n",
        "        cat = torch.cat([s, a], dim=1)\n",
        "        cat = F.leaky_relu(self.fc_cat(cat))\n",
        "\n",
        "        # Output the Q-value\n",
        "        q = self.fc_out(cat)\n",
        "        return q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLVyTe_ef57k"
      },
      "source": [
        "# Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "tOvbvGp9f34s"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer():\n",
        "    def __init__(self, buffer_limit, DEVICE):\n",
        "        self.buffer = deque(maxlen=buffer_limit)\n",
        "        self.dev = DEVICE\n",
        "\n",
        "    def update(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append([r])\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask = 0.0 if done else 1.0\n",
        "            done_mask_lst.append([done_mask])\n",
        "\n",
        "        s_array = np.array(s_lst, dtype=np.float32)\n",
        "        a_array = np.array(a_lst, dtype=np.float32)\n",
        "        r_array = np.array(r_lst, dtype=np.float32)\n",
        "        s_prime_array = np.array(s_prime_lst, dtype=np.float32)\n",
        "        done_array = np.array(done_mask_lst, dtype=np.float32)\n",
        "\n",
        "        s_batch = torch.tensor(s_array, dtype=torch.float).to(self.dev)\n",
        "        a_batch = torch.tensor(a_array, dtype=torch.float).to(self.dev)\n",
        "        r_batch = torch.tensor(r_array, dtype=torch.float).to(self.dev)\n",
        "        s_prime_batch = torch.tensor(s_prime_array, dtype=torch.float).to(self.dev)\n",
        "        done_batch = torch.tensor(done_array, dtype=torch.float).to(self.dev)\n",
        "\n",
        "        # r_batch = (r_batch - r_batch.mean()) / (r_batch.std() + 1e-7)\n",
        "\n",
        "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "AHcfI5kPf7Dr"
      },
      "outputs": [],
      "source": [
        "class SAC:\n",
        "  \"\"\"Implement\"\"\"\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "      self.state_dim      = state_dim  # [cos(theta), sin(theta), theta_dot]\n",
        "      self.action_dim     = action_dim  # [torque] in[-2,2]\n",
        "      self.lr_pi          = 0.001\n",
        "      self.lr_q           = 0.001\n",
        "      self.gamma          = 0.98\n",
        "      self.batch_size     = 200\n",
        "      self.buffer_limit   = 100000\n",
        "      self.tau            = 0.005   # for soft-update of Q using Q-target\n",
        "      self.init_alpha     = 0.01\n",
        "      self.target_entropy = -self.action_dim  # == -1\n",
        "      self.lr_alpha       = 0.005\n",
        "      self.DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "      self.buffer         = ReplayBuffer(self.buffer_limit, self.DEVICE)\n",
        "      print(\"사용 장치 : \", self.DEVICE)\n",
        "      self.total_steps2 = 0\n",
        "      self.warmup_steps = 5000\n",
        "      self.log_alpha = torch.tensor(np.log(self.init_alpha)).to(self.DEVICE)\n",
        "      self.log_alpha.requires_grad = True\n",
        "      self.log_alpha_optimizer = optim.Adam([self.log_alpha], lr=self.lr_alpha)\n",
        "\n",
        "      self.actor  = PolicyNetwork(self.state_dim, self.action_dim, self.lr_pi).to(self.DEVICE)\n",
        "      self.Q1        = QNetwork(self.state_dim, self.action_dim, self.lr_q).to(self.DEVICE)\n",
        "      self.Q1_target = QNetwork(self.state_dim, self.action_dim, self.lr_q).to(self.DEVICE)\n",
        "      self.Q2        = QNetwork(self.state_dim, self.action_dim, self.lr_q).to(self.DEVICE)\n",
        "      self.Q2_target = QNetwork(self.state_dim, self.action_dim, self.lr_q).to(self.DEVICE)\n",
        "\n",
        "      self.Q1_target.load_state_dict(self.Q1.state_dict())\n",
        "      self.Q2_target.load_state_dict(self.Q2.state_dict())\n",
        "\n",
        "  def select_action(self, s):\n",
        "      with torch.no_grad():\n",
        "          action, log_prob = self.actor.sample(s.to(self.DEVICE))\n",
        "      return action, log_prob\n",
        "\n",
        "  def calc_target(self, mini_batch):\n",
        "      s, a, r, s_prime, done = mini_batch\n",
        "      with torch.no_grad():\n",
        "          a_prime, log_prob_prime = self.actor.sample(s_prime)\n",
        "          entropy = - self.log_alpha.exp() * log_prob_prime\n",
        "          q1_target, q2_target = self.Q1_target(s_prime, a_prime), self.Q2_target(s_prime, a_prime)\n",
        "          q_target = torch.min(q1_target, q2_target)\n",
        "          target = r + self.gamma * done * (q_target + entropy)\n",
        "      return target\n",
        "  \n",
        "  def process(self, transition):\n",
        "    self.total_steps2 += 1\n",
        "    self.buffer.update(transition)\n",
        "\n",
        "    if self.total_steps2 > self.warmup_steps:\n",
        "        self.train_agent()\n",
        "\n",
        "    \n",
        "\n",
        "  def train_agent(self):\n",
        "      mini_batch = self.buffer.sample(self.batch_size)\n",
        "      s_batch, a_batch, r_batch, s_prime_batch, done_batch = mini_batch\n",
        "\n",
        "      td_target = self.calc_target(mini_batch)\n",
        "\n",
        "      #### Q1 train ####\n",
        "      q1_loss = F.smooth_l1_loss(self.Q1(s_batch, a_batch), td_target)\n",
        "      self.Q1.optimizer.zero_grad()\n",
        "      q1_loss.mean().backward()\n",
        "      # nn.utils.clip_grad_norm_(self.q1.parameters(), 1.0)\n",
        "      self.Q1.optimizer.step()\n",
        "      #### Q1 train ####\n",
        "\n",
        "      #### Q2 train ####\n",
        "      q2_loss = F.smooth_l1_loss(self.Q2(s_batch, a_batch), td_target)\n",
        "      self.Q2.optimizer.zero_grad()\n",
        "      q2_loss.mean().backward()\n",
        "      # nn.utils.clip_grad_norm_(self.q2.parameters(), 1.0)\n",
        "      self.Q2.optimizer.step()\n",
        "      #### Q2 train ####\n",
        "\n",
        "      #### pi train ####\n",
        "      a, log_prob = self.actor.sample(s_batch)\n",
        "      entropy = -self.log_alpha.exp() * log_prob\n",
        "\n",
        "      q1, q2 = self.Q1(s_batch, a), self.Q2(s_batch, a)\n",
        "      q = torch.min(q1, q2)\n",
        "\n",
        "      pi_loss = -(q + entropy)  # for gradient ascent\n",
        "      self.actor.optimizer.zero_grad()\n",
        "      pi_loss.mean().backward()\n",
        "      # nn.utils.clip_grad_norm_(self.pi.parameters(), 2.0)\n",
        "      self.actor.optimizer.step()\n",
        "      #### pi train ####\n",
        "\n",
        "      #### alpha train ####\n",
        "      self.log_alpha_optimizer.zero_grad()\n",
        "      alpha_loss = -(self.log_alpha.exp() * (log_prob + self.target_entropy).detach()).mean()\n",
        "      alpha_loss.backward()\n",
        "      self.log_alpha_optimizer.step()\n",
        "      #### alpha train ####\n",
        "\n",
        "      #### Q1, Q2 soft-update ####\n",
        "      for param_target, param in zip(self.Q1_target.parameters(), self.Q1.parameters()):\n",
        "          param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
        "      for param_target, param in zip(self.Q2_target.parameters(), self.Q2.parameters()):\n",
        "          param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
        "      #### Q1, Q2 soft-update ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fokausGpJs-"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "fPWOtuvUpLFG"
      },
      "outputs": [],
      "source": [
        "def evaluate(n_evals=1):\n",
        "    eval_env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
        "    eval_env = ImageEnv(eval_env)\n",
        "\n",
        "    scores = 0\n",
        "    for i in range(n_evals):\n",
        "        s, done, ret = eval_env.reset(), False, 0\n",
        "        while not done:\n",
        "            a = agent.select_action(s)\n",
        "            ns, r, done,truncated,_=eval_env.step(a)\n",
        "            ret += r\n",
        "            done = done or truncated\n",
        "        scores += ret\n",
        "    return np.round(scores / n_evals, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNpU-g2RgfBg"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "yygnG8slgw19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "사용 장치 :  cpu\n",
            "step:200 rewards: -940.1107795830923\n",
            "step:400 rewards: -1112.7170185480127\n",
            "step:600 rewards: -1396.5387809978297\n",
            "step:800 rewards: -1412.9879868852315\n",
            "step:1000 rewards: -1244.9905448269205\n",
            "step:1200 rewards: -1309.4382248030486\n",
            "step:1400 rewards: -1368.1377646152857\n",
            "step:1600 rewards: -962.3256417863555\n",
            "step:1800 rewards: -1353.5400821994103\n",
            "step:2000 rewards: -1611.388806853789\n",
            "step:2200 rewards: -1068.9012959524932\n",
            "step:2400 rewards: -1452.6603223232337\n",
            "step:2600 rewards: -1265.9375569674114\n",
            "step:2800 rewards: -1437.393181602782\n",
            "step:3000 rewards: -975.586931869438\n",
            "step:3200 rewards: -1444.9547828726984\n",
            "step:3400 rewards: -1259.5968925057473\n",
            "step:3600 rewards: -873.3939511839313\n",
            "step:3800 rewards: -915.2992095517907\n",
            "step:4000 rewards: -1242.810752810679\n",
            "step:4200 rewards: -1396.2622516635456\n",
            "step:4400 rewards: -1405.0495817819208\n",
            "step:4600 rewards: -969.043860664443\n",
            "step:4800 rewards: -1069.0916007855199\n",
            "step:5000 rewards: -1173.8044054166965\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[172], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m ns, r, done,truncated,_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m     27\u001b[0m total_steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 28\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m s \u001b[38;5;241m=\u001b[39m ns\n\u001b[1;32m     30\u001b[0m rewards\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mr\n",
            "Cell \u001b[0;32mIn[170], line 53\u001b[0m, in \u001b[0;36mSAC.process\u001b[0;34m(self, transition)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mupdate(transition)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_steps2 \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup_steps:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[170], line 88\u001b[0m, in \u001b[0;36mSAC.train_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m pi_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(q \u001b[38;5;241m+\u001b[39m entropy)  \u001b[38;5;66;03m# for gradient ascent\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 88\u001b[0m \u001b[43mpi_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# nn.utils.clip_grad_norm_(self.pi.parameters(), 2.0)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.9/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.9/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/envs/RL/lib/python3.9/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 아래 코드는 예시이며 수정 가능함\n",
        "# 다만,pretrained weight을 저장하는 부분 (actor.pt)는 유지할 것\n",
        "\n",
        "env = gym.make(\"Pendulum-v1\",render_mode=\"rgb_array\")\n",
        "env=ImageEnv(env)\n",
        "\n",
        "max_steps = int(1e6)\n",
        "state_dim = (3, 84, 84)\n",
        "action_dim = 1\n",
        "agent = SAC(state_dim, action_dim)\n",
        "\n",
        "history = {'Step': [], 'AvgReturn': []}\n",
        "s=env.reset()\n",
        "rewards=0\n",
        "score=-1\n",
        "total_steps=0\n",
        "\n",
        "while True:\n",
        "    wandb.init(project='SAC')\n",
        "    wandb.run.name = 'SAC'\n",
        "    wandb.run.save()\n",
        "\n",
        "    if total_steps < 5000:\n",
        "        a = env.action_space.sample()\n",
        "    else:\n",
        "        a, _ = agent.select_action(s)\n",
        "        a = a.cpu().detach().numpy()\n",
        "        a = a[0]\n",
        "    ns, r, done,truncated,_ = env.step(a)\n",
        "    total_steps+=1\n",
        "    agent.process((s, a, r, ns, done))\n",
        "    s = ns\n",
        "    rewards+=r\n",
        "    \n",
        "\n",
        "    if done or truncated:\n",
        "        s=env.reset()\n",
        "        print(\"step:{} rewards: {}\".format(total_steps,rewards))\n",
        "        history['Step'].append(total_steps)\n",
        "        history['rewards'].append(rewards)\n",
        "        wandb.log({'Step': total_steps, 'AvgReturn': rewards})\n",
        "        if score < rewards:\n",
        "            score = rewards\n",
        "            torch.save(agent.actor.state_dict(), 'actor.pt')\n",
        "            print(\"New Best Saved\")\n",
        "\n",
        "        rewards=0\n",
        "\n",
        "    if total_steps > max_steps:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GXH5x4dgylf"
      },
      "source": [
        "# Visualize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEIAXfsTgzPO"
      },
      "outputs": [],
      "source": [
        "!mkdir video\n",
        "\n",
        "state_dim = (3, 84, 84)\n",
        "action_dim = 1\n",
        "agent = SAC(state_dim, action_dim)\n",
        "\n",
        "def show_video(env_name):\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = 'video/{}.mp4'.format(env_name)\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        return HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii')))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "def save_video_of_model(policy, env_name):\n",
        "\n",
        "\n",
        "    env = gym.make(env_name,render_mode=\"rgb_array\")\n",
        "    env=ImageEnv(env)\n",
        "    agent.actor.load_state_dict(torch.load('/content/actor.pt'))\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    frames=[]\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        a = agent.select_action(s)\n",
        "        ns, r, done,truncated,_ = env.step(a)\n",
        "        done = done or truncated\n",
        "        s=ns\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave(\"video/{}.mp4\".format(env_name), frames, fps=30)\n",
        "\n",
        "save_video_of_model(agent, 'Pendulum-v1')\n",
        "show_video('Pendulum-v1')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
